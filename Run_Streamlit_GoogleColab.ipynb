{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmc_s2ezvU0"
      },
      "source": [
        "# Run streamlit app from a Google Colab Notebook\n",
        "> The template is created by [Manuel Romero](https://twitter.com/mrm8488)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "QGzP3hEvWYmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c375959-aceb-412a-8fb7-c7a32e026dcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/gdrive/MyDrive/faiss/temp.zip' -d '/content/gdrive/MyDrive/faiss'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LqdwA8ldM94",
        "outputId": "5914e7b7-94c6-4e09-cc5f-07fa355cab3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/faiss/temp.zip\n",
            "   creating: /content/gdrive/MyDrive/faiss/faiss_index_all-mpnet-base-v2_cs500_co50_1000/\n",
            "  inflating: /content/gdrive/MyDrive/faiss/faiss_index_all-mpnet-base-v2_cs500_co50_1000/index.pkl  \n",
            "  inflating: /content/gdrive/MyDrive/faiss/faiss_index_all-mpnet-base-v2_cs500_co50_1000/index.faiss  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvlYkCQ9vFiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080df7c1-4a4e-4dfd-9a96-35a6c7393587"
      },
      "source": [
        "!pip install -q streamlit streamlit-chat"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle langchain torch transformers faiss-gpu sentence-transformers pandas numpy"
      ],
      "metadata": {
        "id": "an2C3c_0Ooi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c61e169-d21b-4c8c-b42f-fb2e53870442"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes trl optimum auto-gptq ragas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YykwYpKtL_Oh",
        "outputId": "6484c472-8eee-4918-91cb-a1463b875bdd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waCfwniZOow8"
      },
      "source": [
        "## Create a streamlit app example\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag.py\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import AutoTokenizer, pipeline, AutoConfig, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import format_document\n",
        "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def define_embedding(model_path:str,\n",
        "                     model_kwargs:dict,\n",
        "                     encode_kwargs:dict):\n",
        "\n",
        "    '''\n",
        "    model_path: define the path to the pre-trained model you want to use\n",
        "    model_kwargs: dictionary with model configuration options, specifying to use the CPU/GPU for computations\n",
        "    encode_kwargs: dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "    '''\n",
        "\n",
        "    # Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=model_path,     # Provide the pre-trained model's path\n",
        "        model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "        encode_kwargs=encode_kwargs # Pass the encoding options\n",
        "    )\n",
        "\n",
        "def load_dataset(file_path:str,\n",
        "                 source_column:str,\n",
        "                 type_file=\"csv\",\n",
        "                 chunk_size=300,\n",
        "                 chunk_overlap=0):\n",
        "    '''\n",
        "    file_path: define the path of the dataset that you want to use\n",
        "    source_column:\n",
        "    type_file:\n",
        "    chunk_size:\n",
        "    chunk_overlap:\n",
        "    '''\n",
        "    if type_file == \"csv\":\n",
        "        loader = CSVLoader(file_path=file_path, source_column=source_column)\n",
        "        data = loader.load()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        docs = text_splitter.split_documents(data)\n",
        "        for doc in docs:\n",
        "            doc.page_content = doc.page_content.split(\"\\nclean_review_text: \")[-1]\n",
        "        return docs\n",
        "    elif type_file == \"dataframe\":\n",
        "        loader = DataFrameLoader(pd.read_csv(file_path), page_content_column=\"clean_review_text\")\n",
        "        data = loader.load()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        docs = text_splitter.split_documents(data)\n",
        "        return docs\n",
        "    else:\n",
        "         raise ValueError('The extension of the source should be .csv or dataframe')\n",
        "\n",
        "def load_faiss_db(db_path,\n",
        "                  embeddings,\n",
        "                  docs=None,\n",
        "                  is_visualize=False):\n",
        "    '''\n",
        "    db_path: path that indicate index db\n",
        "    embeddings:\n",
        "    '''\n",
        "    db = FAISS.load_local(db_path, embeddings=embeddings, distance_strategy=DistanceStrategy.COSINE)\n",
        "    return db\n",
        "\n",
        "def define_llm(model_name:str,\n",
        "               use_4bit:bool,\n",
        "               disable_exllama=None):\n",
        "    \"\"\"\n",
        "    model_name: the name of available online LLMs which are available to use\n",
        "    use_4bit: flag for activate the quantization by defining new settings\n",
        "    \"\"\"\n",
        "    model_config = AutoConfig.from_pretrained(\n",
        "        model_name,\n",
        "    )\n",
        "\n",
        "    if disable_exllama != None:\n",
        "        model_config.quantization_config[\"disable_exllama\"] = False\n",
        "        model_config.quantization_config[\"exllama_config\"] = {\"version\":2}\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    if use_4bit:\n",
        "        bnb_4bit_compute_dtype = \"float16\"\n",
        "        bnb_4bit_quant_type = \"nf4\"\n",
        "        use_nested_quant = False\n",
        "\n",
        "        compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=use_4bit,\n",
        "            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_use_double_quant=use_nested_quant,\n",
        "        )\n",
        "\n",
        "        if compute_dtype == torch.float16 and use_4bit:\n",
        "            major, _ = torch.cuda.get_device_capability()\n",
        "            if major >= 8:\n",
        "                print(\"=\" * 80)\n",
        "                print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "                print(\"=\" * 80)\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "        )\n",
        "    elif (not use_4bit) and device == torch.device(\"cuda\"):\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "            )\n",
        "        except:\n",
        "             model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"cuda:0\",\n",
        "                config=model_config\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError('Available device is cpu')\n",
        "    return model, tokenizer\n",
        "\n",
        "def create_response_chain(model, tokenizer, max_new_tokens, temperature=0.2,\n",
        "                          repetition_penalty=1.1, return_full_text=True,\n",
        "                          ):\n",
        "    \"\"\"\n",
        "    model: llm that is used\n",
        "    tokenizer: tokenizer that is used\n",
        "    max_new_tokens: the limit of generated answer will be generated\n",
        "    \"\"\"\n",
        "    text_generation_pipeline = pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        temperature=temperature,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        return_full_text=return_full_text,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    ### [INST]\n",
        "    You are a helpful, respectful and honest assistant.\n",
        "    Always answer as helpfully as possible, while being safe.\n",
        "    Answer the question based on OUR APPLICATION knowledge from the context.\n",
        "    OUR APPLICATION is SPOTIFY. If a question does not make any sense, or is not factually coherent,\n",
        "    explain why instead of answering something not correct.\n",
        "    If you don't know the answer to a question, please don't share false information.\n",
        "    Here is context to help:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    ### QUESTION:\n",
        "    {question}\n",
        "\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "\n",
        "    chosen_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "    # Create prompt from prompt template\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=prompt_template,\n",
        "    )\n",
        "\n",
        "    # Create llm chain\n",
        "    return LLMChain(llm=chosen_llm, prompt=prompt)\n",
        "\n",
        "def answer_with_rag(question: str,\n",
        "                    llm_chain: LLMChain,\n",
        "                    db,\n",
        "                    k_retrieve:int=4):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    retriever = db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                search_kwargs={\"k\": k_retrieve,\n",
        "                                              \"score_threshold\":0.2})\n",
        "    rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "                 | llm_chain)\n",
        "    result = rag_chain.invoke(question)\n",
        "    relevant_docs = [doc.page_content for doc in result[\"context\"]]\n",
        "    return result[\"text\"], relevant_docs"
      ],
      "metadata": {
        "id": "FlEaz-e0Lk-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1de0dd-8775-4029-9475-0f58fbc3bf42"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from streamlit_chat import message as st_message\n",
        "from rag import *\n",
        "\n",
        "model_kwargs = {'device': \"cuda\"}\n",
        "encode_kwargs = {'normalize_embeddings': \"True\"}\n",
        "embeddings = define_embedding(\"sentence-transformers/all-mpnet-base-v2\", model_kwargs, encode_kwargs)\n",
        "\n",
        "db = load_faiss_db(\"/content/gdrive/MyDrive/faiss/faiss_index_all-mpnet-base-v2_cs500_co50_1000\", embeddings=embeddings)\n",
        "model, tokenizer = define_llm(\"mistralai/Mistral-7B-Instruct-v0.1\", True)\n",
        "llm_chain = create_response_chain(model, tokenizer, 256, temperature=0.1)\n",
        "\n",
        "# Set up the Streamlit app\n",
        "st.title(\"ðŸ¤– Chat with your Website\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    ####  ðŸ—¨ï¸ Chat with your a website ðŸ“œ\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "if 'generated' not in st.session_state:\n",
        "    st.session_state['generated'] = [\"Hello ! Ask me anything about this website ðŸ¤—\"]\n",
        "\n",
        "if 'past' not in st.session_state:\n",
        "    st.session_state['past'] =  [\"Hey ! ðŸ‘‹\"]\n",
        "\n",
        "#container for the chat history and user's text input\n",
        "response_container, container = st.container(), st.container()\n",
        "\n",
        "with container:\n",
        "    with st.form(key='my_form', clear_on_submit=True):\n",
        "        # Allow the user to enter a query and generate a response\n",
        "        user_input  = st.text_input(\n",
        "            \"**Talk with your website here**\",\n",
        "            placeholder=\"Talk with your website here.\",\n",
        "        )\n",
        "        submit_button = st.form_submit_button(label='Send')\n",
        "\n",
        "        if user_input:\n",
        "            with st.spinner(\n",
        "                \"Generating Answer to your Query : `{}` \".format(user_input )\n",
        "            ):\n",
        "                answer, relevant_docs = answer_with_rag(user_input, llm_chain, db, k_retrieve=10)\n",
        "\n",
        "                source = []\n",
        "                for x in relevant_docs:\n",
        "                    source.append(x)\n",
        "                    print(x)\n",
        "\n",
        "                st.session_state['past'].append(user_input)\n",
        "                st.session_state['generated'].append(answer)\n",
        "\n",
        "if st.session_state['generated']:\n",
        "    with response_container:\n",
        "        for i in range(len(st.session_state['generated'])):\n",
        "            st_message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"fun-emoji\")\n",
        "            st_message(st.session_state[\"generated\"][i], key=str(i), avatar_style=\"croodles-neutral\")"
      ],
      "metadata": {
        "id": "meJ36PefNftd",
        "outputId": "3e2cd553-875a-4035-c7ba-04c3e8bf18ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZlEJkmSOoxC"
      },
      "source": [
        "## Install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c5ea25f-66e7-4aa2-de75-a2083267c05d",
        "id": "ZAyqQCQVOoxC"
      },
      "source": [
        "!npm install localtunnel"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.972s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "btIiK_3llWDH",
        "outputId": "1af5d90d-269c-4e3d-a67b-474caf65e79b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.173.65.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run streamlit in background"
      ],
      "metadata": {
        "id": "kccYE2lkN20y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "Zv912rRAN0fs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expose the port 8501\n",
        "Then just click in the `url` showed.\n",
        "\n",
        "A `log.txt`file will be created."
      ],
      "metadata": {
        "id": "h_KW9juhOCuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "XTGAizLhOIgC",
        "outputId": "2bfe866e-77eb-4914-d536-ff4ed68d824a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.004s\n",
            "your url is: https://floppy-bushes-agree.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVz-H__pOoxG"
      },
      "source": [
        "[![ko-fi](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/Y8Y3VYYE)"
      ]
    }
  ]
}